{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMM+t/1i7jcrjjPNyPzpxVc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import requests\n","import time\n","import glob\n","import os"],"metadata":{"id":"wle5eqwtvkhX","executionInfo":{"status":"ok","timestamp":1744389968421,"user_tz":-330,"elapsed":1561,"user":{"displayName":"Harsh Kumar Tyagi","userId":"10904021106534997984"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Flight Data Aggregation and Date Range Splitting for Weather Data Fetching**"],"metadata":{"id":"Rl7TFCbqwF6-"}},{"cell_type":"code","source":["\"\"\"\n","  After running this code, we will have a CSV file (`split_df.csv`) with city names, date ranges,\n","  and flags to indicate which date ranges still need weather data. This will help us fetch\n","  weather data for specific cities and date ranges.\n","\"\"\"\n","\n","flight_df = pd.read_csv('./data/flights_sample_3m.csv')\n","\n","a_df = flight_df.groupby(['ORIGIN_CITY']).agg({\n","    'FL_DATE': ['nunique', 'min', 'max']  # Get the unique count, min, and max of flight dates\n","}).reset_index()\n","a_df.columns = ['origin_city', 'nunique', 'fl_dt_min', 'fl_dt_max']\n","\n","# Extract city name and city code from 'origin_city'\n","a_df['city_name'] = a_df['origin_city'].map(\n","    lambda x: x.split(',')[0].split('/')[0]  # Extract the city name from 'origin_city'\n",")\n","\n","a_df['city_code'] = a_df['origin_city'].map(\n","    lambda x: x.split(',')[1]  # Extract the city code from 'origin_city'\n",")\n","\n","a_df[\"fl_dt_min\"] = a_df[\"fl_dt_min\"].astype(str)\n","a_df[\"fl_dt_max\"] = a_df[\"fl_dt_max\"].astype(str)\n","\n","a_df['fl_dt_min'] = pd.to_datetime(a_df['fl_dt_min'])\n","a_df['fl_dt_max'] = pd.to_datetime(a_df['fl_dt_max'])\n","a_df['day_diff'] = (a_df['fl_dt_max'] - a_df['fl_dt_min']).dt.days\n","\n","# Split the date ranges into smaller chunks if the range exceeds a threshold\n","split_rows = []\n","THRESHOLD = 400  # The maximum number of days per chunk\n","\n","for idx, row in a_df.iterrows():\n","    remaining_days = row['day_diff']\n","    current_start = row['fl_dt_min']\n","\n","    while remaining_days > THRESHOLD:\n","        split_rows.append({\n","            'origin_city': row['origin_city'],\n","            'city_name': row['city_name'],\n","            'fl_dt_min': current_start,\n","            'fl_dt_max': current_start + pd.Timedelta(days=THRESHOLD),\n","            'day_diff': THRESHOLD\n","        })\n","        remaining_days -= (THRESHOLD + 1)\n","\n","    # Add the final chunk if any remaining days are less than the threshold\n","    if remaining_days > 0:\n","        split_rows.append({\n","            'origin_city': row['origin_city'],\n","            'city_name': row['city_name'],\n","            'fl_dt_min': current_start,\n","            'fl_dt_max': row['fl_dt_max'],\n","            'day_diff': (row['fl_dt_max'] - current_start).days\n","        })\n","\n","split_df = pd.DataFrame(split_rows)\n","\n","split_df['flg_fetch'] = 0  # Flag to indicate if weather data has been fetched\n","split_df['start_date'] = split_df['fl_dt_min']  # Start date of the chunk\n","split_df['end_date'] = split_df['fl_dt_max']  # End date of the chunk\n","\n","split_df.to_csv('./split_df.csv', index=False)"],"metadata":{"id":"DZrX8jx1vmfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m5L5fIMXyiqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Weather Data Fetching Script using Visual Crossing API**"],"metadata":{"id":"ZEB6BYF4yjiA"}},{"cell_type":"code","source":["\"\"\"\n","    This script fetches weather data for cities and date ranges specified in the split_df file.\n","    It avoids fetching duplicate data by ensuring the 'flg_fetch' column is 0 (indicating weather data has not been fetched yet).\n","    Weather data is retrieved using the Visual Crossing API and stored as CSV files for each city and date range.\n","    After fetching, the 'flg_fetch' column is updated to 1 to indicate that the weather data for that specific period has been fetched.\n","\"\"\"\n","\n","import pandas as pd\n","import requests\n","import time\n","\n","\n","split_df = pd.read_csv('./split_df.csv')\n","\n","# Filter out the rows that haven't had weather data fetched yet (flg_fetch == 0)\n","split_df = split_df[split_df['flg_fetch'] == 0]\n","\n","\n","def get_weather_bulk(city, start_date, end_date, api_key):\n","    BASE_URL = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/\"\n","    url = f\"{BASE_URL}{city}/{start_date}/{end_date}?key={api_key}&include=days&contentType=json\"\n","\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        data = response.json().get(\"days\", [])\n","        if data:\n","            df = pd.DataFrame(data)\n","            df['city'] = city\n","            df['start_date'] = start_date\n","            df['end_date'] = end_date\n","            return df\n","        else:\n","            print(f\"No data available for {city} between {start_date} and {end_date}.\")\n","            return pd.DataFrame()\n","    else:\n","        print(f\"Error fetching data for {city}: HTTP {response.status_code} - {response.text}\")\n","        return pd.DataFrame()\n","\n","# List of API keys for authentication with the Visual Crossing API\n","api_keys =  ['ELDKA2BG6NKRWNDM5776PVRPL', 'EYT2VCQPMAXV6ZK77MWRCZBN7', 'ZN4Y72MP3G5S98RQZ8M9LN2BE',\n","             'AVEMEQN8ZL45N27TQF7RQDJGQ', '5CHTDUCBRZBKR5ZFL3HC2VB75', 'U6WVN4A72JUEEW3N7SLZXM7CN',\n","             '7FVJZKNA9J7B4RV4VSDF3YLM7', 'E7JRB82RB5LND8SDG6NCR4LEB', '68G8M3YHM5E29DA7C2Y7AFKUY',\n","             'P427H269UM2TERXN6TK2XKMAG', 'QFP292BSQWU5KNCQWFZGCPSCQ', 'JLK2CTBFUGFYVRD4L7LW3B3K9',\n","             'VDMDB4UW3M8QXNFWE7W2JEMGT', 'ABD6CU8YX59M77DMUPV6BURKB', 'WJHYBMW5GKBPHHX23E5MPTJR9',\n","             'KVA5FXXK97LNPM9J7JXHR8KXL', 'PYLL32BJ6GQEY7E3CQDEWSB9V', '5JLE7QCYC28GARPDETXWZ67ZV',\n","             'WTFP829GRTQC3XXJYTBFMRHBK', 'F8D3NZPVTVUEJ9RHPAGRU5KP5', '4EXDMKUJYAKUAPD9ZBE99CG8P',\n","             'L767TUTXJ7N4AAF4CF3PM6PFW', '3QCMGWNLNPUVDUFGTASLMPEFL', 'LZWNLC9PKC2VCE5KBG9MKDMR2',\n","             'P9EAPE7W2VHGRQRZ4248F4KJC']\n","\n","\n","i = 0\n","dummy_df = pd.DataFrame()\n","\n","while i < len(api_keys):\n","    print(f\"Using API key: {api_keys[i]}\")\n","    split_df = split_df[split_df['flg_fetch'] == 0]\n","\n","    for idx, row in split_df.iterrows():\n","        print(f\"Fetching weather data for {row['city_name']} from {row['start_date']} to {row['end_date']}\")\n","        df = get_weather_bulk(row['city_name'], row['start_date'], row['end_date'], api_keys[i])\n","\n","\n","        time.sleep(1)\n","\n","        if not df.empty:\n","            df.to_csv(f'./weather_data/{row[\"city_name\"]}_{row[\"start_date\"]}_{row[\"end_date\"]}.csv')\n","            split_df.at[idx, 'flg_fetch'] = 1\n","        else:\n","            print(f\"Received empty data for {row['city_name']} between {row['start_date']} and {row['end_date']}.\")\n","            i += 1\n","            break\n","\n","\n","split_df.to_csv('./split_df.csv', index=False)\n","\n","\"\"\"\n","At this point, the weather data has been fetched and saved for the specified cities and date ranges,\n","and the split_df file has been updated to reflect the progress.\n","\"\"\""],"metadata":{"id":"88RYSmUWwwBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"diqxO4SvypQ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Merging Weather Data Files into a Single CSV for Analysis**"],"metadata":{"id":"chlsBbL1zrRv"}},{"cell_type":"code","source":["\"\"\"\n","Now that we have gathered all the weather data in individual files, we will combine them into a single file for analysis purposes.\n","\"\"\"\n","folder_path = './weather_data/'\n","\n","csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n","\n","dfs = []\n","i = 0\n","for file in csv_files:\n","    df = pd.read_csv(file)\n","    df['SourceFile'] = os.path.basename(file)  # Add a column to track the source file\n","    dfs.append(df)\n","    i += 1\n","    print(f\"Processing file {i}: {file}\")\n","\n","\n","combined_df = pd.concat(dfs, ignore_index=True)\n","\n","\n","combined_df.to_csv('/content/drive/MyDrive/data_mining/project/data/combined_weather_data.csv')\n"],"metadata":{"id":"6aEs-LI4yp8z"},"execution_count":null,"outputs":[]}]}